{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\neela\\desktop\\sns_ihub assesment\\.venv\\lib\\site-packages (3.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\neela\\desktop\\sns_ihub assesment\\.venv\\lib\\site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\neela\\desktop\\sns_ihub assesment\\.venv\\lib\\site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\neela\\desktop\\sns_ihub assesment\\.venv\\lib\\site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\neela\\desktop\\sns_ihub assesment\\.venv\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\neela\\desktop\\sns_ihub assesment\\.venv\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\neela\\desktop\\sns_ihub assesment\\.venv\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\neela\\desktop\\sns_ihub assesment\\.venv\\lib\\site-packages (from datasets) (4.66.6)\n",
      "Requirement already satisfied: xxhash in c:\\users\\neela\\desktop\\sns_ihub assesment\\.venv\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\neela\\desktop\\sns_ihub assesment\\.venv\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in c:\\users\\neela\\desktop\\sns_ihub assesment\\.venv\\lib\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\neela\\desktop\\sns_ihub assesment\\.venv\\lib\\site-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\neela\\desktop\\sns_ihub assesment\\.venv\\lib\\site-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\neela\\desktop\\sns_ihub assesment\\.venv\\lib\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\neela\\desktop\\sns_ihub assesment\\.venv\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\neela\\desktop\\sns_ihub assesment\\.venv\\lib\\site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\neela\\desktop\\sns_ihub assesment\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\neela\\desktop\\sns_ihub assesment\\.venv\\lib\\site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\neela\\desktop\\sns_ihub assesment\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\neela\\desktop\\sns_ihub assesment\\.venv\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\neela\\desktop\\sns_ihub assesment\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.17.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\neela\\desktop\\sns_ihub assesment\\.venv\\lib\\site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\neela\\desktop\\sns_ihub assesment\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\neela\\desktop\\sns_ihub assesment\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\neela\\desktop\\sns_ihub assesment\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\neela\\desktop\\sns_ihub assesment\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\neela\\desktop\\sns_ihub assesment\\.venv\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\neela\\desktop\\sns_ihub assesment\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\neela\\desktop\\sns_ihub assesment\\.venv\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\neela\\desktop\\sns_ihub assesment\\.venv\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\neela\\desktop\\sns_ihub assesment\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\neela\\desktop\\sns_ihub assesment\\.venv\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\neela\\Desktop\\SNS_IHUB Assesment\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Your max_length is set to 50, but your input_length is only 35. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=17)\n",
      "Your max_length is set to 50, but your input_length is only 35. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=17)\n",
      "Your max_length is set to 50, but your input_length is only 36. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=18)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 1: Summarize the following: \n",
      "Generated Summary:  Machine learning is a field of artificial intelligence that focuses on the development of algorithms that allow computers to learn from and make predictions on data .\n",
      "\n",
      "Prompt 2: What is the main point? \n",
      "Generated Summary:  Machine learning is a field of artificial intelligence that focuses on the development of algorithms that allow computers to learn from and make predictions on data .\n",
      "\n",
      "Prompt 3: In brief, tell me about: \n",
      "Generated Summary:  Machine learning is a field of artificial intelligence that focuses on the development of algorithms that allow computers to learn from and make predictions on data .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "# from datasets import load_metric\n",
    "\n",
    "# Initialize summarization pipeline\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "# Define prompts and texts\n",
    "prompts = [\n",
    "    \"Summarize the following: \",\n",
    "    \"What is the main point? \",\n",
    "    \"In brief, tell me about: \"\n",
    "]\n",
    "text = \"Machine learning is a field of artificial intelligence that focuses on the development of algorithms that allow computers to learn from and make predictions on data.\"\n",
    "\n",
    "# Test different prompts\n",
    "results = []\n",
    "for prompt in prompts:\n",
    "    generated_summary = summarizer(prompt + text, max_length=50, min_length=25, do_sample=False)\n",
    "    results.append(generated_summary[0][\"summary_text\"])\n",
    "\n",
    "# Display results\n",
    "for i, summary in enumerate(results):\n",
    "    print(f\"Prompt {i+1}: {prompts[i]}\")\n",
    "    print(f\"Generated Summary: {summary}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting absl-py\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: nltk in c:\\users\\neela\\desktop\\sns_ihub assesment\\.venv\\lib\\site-packages (from rouge_score) (3.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\neela\\desktop\\sns_ihub assesment\\.venv\\lib\\site-packages (from rouge_score) (2.0.2)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\neela\\desktop\\sns_ihub assesment\\.venv\\lib\\site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\neela\\desktop\\sns_ihub assesment\\.venv\\lib\\site-packages (from nltk->rouge_score) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\neela\\desktop\\sns_ihub assesment\\.venv\\lib\\site-packages (from nltk->rouge_score) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\neela\\desktop\\sns_ihub assesment\\.venv\\lib\\site-packages (from nltk->rouge_score) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\neela\\desktop\\sns_ihub assesment\\.venv\\lib\\site-packages (from nltk->rouge_score) (4.66.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\neela\\desktop\\sns_ihub assesment\\.venv\\lib\\site-packages (from click->nltk->rouge_score) (0.4.6)\n",
      "Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (pyproject.toml): started\n",
      "  Building wheel for rouge_score (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24974 sha256=bbf5791b74ec826c484404af2b66cc8799343999a589d8ad48bc3ef96c84d50b\n",
      "  Stored in directory: c:\\users\\neela\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local\\pip\\cache\\wheels\\85\\9d\\af\\01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: absl-py, rouge_score\n",
      "Successfully installed absl-py-2.1.0 rouge_score-0.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install rouge_score absl-py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import evaluate\n",
    "import torch\n",
    "\n",
    "# Initialize summarization or question-answering pipeline\n",
    "task = \"summarization\"  # Use \"question-answering\" for QA\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "model_pipeline = pipeline(task, model=\"sshleifer/distilbart-cnn-12-6\", device=device)\n",
    "\n",
    "# Define evaluation metric\n",
    "rouge = evaluate.load(\"rouge\")  # For summarization tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prompts for summarization\n",
    "prompts = [\n",
    "    \"Summarize the following in one sentence: \",\n",
    "    \"Provide a brief overview: \",\n",
    "    \"Can you explain the main points in a concise way? \",\n",
    "    \"What is the key takeaway? \"\n",
    "]\n",
    "\n",
    "# Sample text to summarize\n",
    "text = \"\"\"\n",
    "AI refers to the development of programs that behave intelligently and mimic human intelligence through a set of algorithms. The field focuses on three skills: learning, reasoning, and self-correction to obtain maximum efficiency. AI can refer to either machine learning-based programs or even explicitly programmed computer programs.\n",
    "\n",
    "Machine learning is a subset of AI, which uses algorithms that learn from data to make predictions. These predictions can be generated through supervised learning, where algorithms learn patterns from existing data, or unsupervised learning, where they discover general patterns in data. ML models can predict numerical values based on historical data, categorize events as true or false, and cluster data points based on commonalities.\n",
    "\n",
    "Deep learning, on the other hand, is a subfield of machine learning dealing with algorithms based essentially on multi-layered artificial neural networks (ANN) that are inspired by the structure of the human brain.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 1: Summarize the following in one sentence: \n",
      "Generated Summary:  AI refers to the development of programs that behave intelligently and mimic human intelligence through a set of algorithms . The field focuses on three skills: learning, reasoning, and self-correction to obtain maximum efficiency . Machine learning is a subset\n",
      "\n",
      "Prompt 2: Provide a brief overview: \n",
      "Generated Summary:  The field focuses on three skills: learning, reasoning, and self-correction . Machine learning is a subset of AI, which uses algorithms that learn from data to make predictions . Deep learning, on the other hand, is a sub\n",
      "\n",
      "Prompt 3: Can you explain the main points in a concise way? \n",
      "Generated Summary:  AI refers to the development of programs that behave intelligently and mimic human intelligence through a set of algorithms . The field focuses on three skills: learning, reasoning, and self-correction to obtain maximum efficiency .\n",
      "\n",
      "Prompt 4: What is the key takeaway? \n",
      "Generated Summary:  The field focuses on three skills: learning, reasoning, and self-correction . Machine learning is a subset of AI, which uses algorithms that learn from data to make predictions . Deep learning, on the other hand, is a sub\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for prompt in prompts:\n",
    "    input_text = prompt + text\n",
    "    summary = model_pipeline(input_text, max_length=50, min_length=25, do_sample=False)\n",
    "    results.append(summary[0][\"summary_text\"])\n",
    "\n",
    "# Display results\n",
    "for i, summary in enumerate(results):\n",
    "    print(f\"Prompt {i+1}: {prompts[i]}\")\n",
    "    print(f\"Generated Summary: {summary}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 1 ROUGE Scores: {'rouge1': np.float64(0.1694915254237288), 'rouge2': np.float64(0.03508771929824561), 'rougeL': np.float64(0.1016949152542373), 'rougeLsum': np.float64(0.1016949152542373)}\n",
      "Prompt 2 ROUGE Scores: {'rouge1': np.float64(0.2807017543859649), 'rouge2': np.float64(0.1090909090909091), 'rougeL': np.float64(0.2105263157894737), 'rougeLsum': np.float64(0.2105263157894737)}\n",
      "Prompt 3 ROUGE Scores: {'rouge1': np.float64(0.11111111111111112), 'rouge2': np.float64(0.0), 'rougeL': np.float64(0.07407407407407408), 'rougeLsum': np.float64(0.07407407407407408)}\n",
      "Prompt 4 ROUGE Scores: {'rouge1': np.float64(0.2807017543859649), 'rouge2': np.float64(0.1090909090909091), 'rougeL': np.float64(0.2105263157894737), 'rougeLsum': np.float64(0.2105263157894737)}\n"
     ]
    }
   ],
   "source": [
    "# Reference summary for evaluation\n",
    "reference_summary = [\"Machine learning enables computers to learn from data and is used in various fields, making it crucial for modern tech.\"]\n",
    "\n",
    "# Compute and display ROUGE scores\n",
    "for i, summary in enumerate(results):\n",
    "    scores = rouge.compute(predictions=[summary], references=reference_summary)\n",
    "    print(f\"Prompt {i+1} ROUGE Scores: {scores}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
