{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install spacy nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\neela\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Tokens: ['quick', 'brown', 'fox', 'jump', 'lazy', 'dog', 'despite', 'rain', 'enjoy', 'adventure', 'nlp', 'tool', 'like', 'nltk', 'spacy', 'make', 'text', 'process', 'quite', 'effective', 'however', 'preprocessing', 'require', 'handle', 'case', 'stop', 'word', 'punctuation', 'sometimes', 'stem', 'lemmatization']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "# Load spaCy's English model\n",
    "model_nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Download stopwords if not already done\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses and tokenizes input text by handling cases, punctuation,\n",
    "    stop words, and lemmatization.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The input text to preprocess.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of processed, tokenized words.\n",
    "    \"\"\"\n",
    "    # Step 1: Lowercase the text\n",
    "    text = text.lower()\n",
    "\n",
    "    # Step 2: Process text using spaCy's NLP pipeline\n",
    "    doc = model_nlp(text)\n",
    "    \n",
    "    # Step 3: Filter tokens\n",
    "    # - Exclude stop words and punctuation\n",
    "    # - Keep only alphabetic tokens\n",
    "    tokens = [\n",
    "        token.lemma_ for token in doc\n",
    "        if token.lemma_ not in stop_words and\n",
    "           token.is_alpha                    # Exclude punctuation and numbers\n",
    "    ]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Example usage\n",
    "#sample_text = \"The Quick brown Foxes were jumping over the lazy dogs! Despite the rain, they enjoyed the adventure. NLP tools, like NLTK and spaCy, make text processing quite effective. However, the preprocessing requires handling cases, stop words, punctuation, and sometimes stemming or lemmatization.\"\n",
    "sample_text=input(\"Enter the Text for preprocess and tokenize text: \")\n",
    "processed_tokens = preprocess_text(sample_text)\n",
    "print(\"Processed Tokens:\", processed_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
